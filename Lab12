import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, utils
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)


batch_size = 32
image_size = 32
nz = 32        # latent vector
ngf = 16       # generator feature maps
ndf = 16       # discriminator feature maps
nc = 3
epochs = 1     # demo, 1 epoch
lr = 0.0002
beta1 = 0.5

transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
])

dataset = datasets.CIFAR10(root='data', download=True, transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False),   # 1x1 -> 4x4
            nn.BatchNorm2d(ngf*4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False), # 4x4 -> 8x8
            nn.BatchNorm2d(ngf*2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),   # 8x8 ->16x16
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),     #16x16->32x32
            nn.Tanh()
        )

    def forward(self, x):
        return self.main(x)


class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),   #32->16
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),#16->8
            nn.BatchNorm2d(ndf*2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),#8->4
            nn.BatchNorm2d(ndf*4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False),  #4->1
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x).view(-1)  # now outputs [batch_size]

netG = Generator().to(device)
netD = Discriminator().to(device)

# Weight initialization
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

netG.apply(weights_init)
netD.apply(weights_init)


criterion = nn.BCELoss()
optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1,0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1,0.999))
fixed_noise = torch.randn(16, nz, 1, 1, device=device)  # 16 images preview

G_losses, D_losses = [], []

for epoch in range(epochs):
    for i, (imgs, _) in enumerate(dataloader):
        real_imgs = imgs.to(device)
        b_size = real_imgs.size(0)

        # Train Discriminator
        netD.zero_grad()
        label = torch.full((b_size,),1.,dtype=torch.float,device=device)
        lossD_real = criterion(netD(real_imgs), label)
        lossD_real.backward()

        noise = torch.randn(b_size, nz,1,1,device=device)
        fake_imgs = netG(noise)
        label.fill_(0.)
        lossD_fake = criterion(netD(fake_imgs.detach()), label)
        lossD_fake.backward()
        optimizerD.step()
        lossD = lossD_real + lossD_fake

        # Train Generator
        netG.zero_grad()
        label.fill_(1.)
        lossG = criterion(netD(fake_imgs), label)
        lossG.backward()
        optimizerG.step()

        # Save losses
        G_losses.append(lossG.item())
        D_losses.append(lossD.item())

        # Instant visualization every 50 iterations
        if i % 50 == 0:
            print(f"Epoch {epoch+1} Iter {i}/{len(dataloader)} Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f}")
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            plt.figure(figsize=(4,4))
            plt.axis('off')
            plt.title(f"Epoch {epoch+1} Iter {i}")
            plt.imshow(utils.make_grid(fake, nrow=4, normalize=True).permute(1,2,0))
            plt.show()

plt.figure(figsize=(8,4))
plt.title("Generator and Discriminator Loss")
plt.plot(G_losses, label="Generator")
plt.plot(D_losses, label="Discriminator")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()
